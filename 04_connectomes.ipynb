{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp connectomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module connectomes\n",
    "---\n",
    "\n",
    "Pipeline making use of `postprocessing` nodes to generate a connectivity matrix and distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201001-10:23:12,863 nipype.utils WARNING:\n",
      "\t A newer version (1.5.1) of nipy/nipype is available. You are using 1.5.0\n"
     ]
    }
   ],
   "source": [
    "from nipype import IdentityInterface\n",
    "from nipype.pipeline import Node, Workflow\n",
    "from nipype.interfaces.io import SelectFiles, DataSink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class connectome:\n",
    "    \"\"\"\n",
    "    Create a workflow that produces connectomes based on input atlases and streamlines\n",
    "    \n",
    "    Inputs:\n",
    "    atlas_dir (str): base directory of folder containing atlases\n",
    "    atlas_list (List of strings): names of atlases: aal, brainnectome, desikan-killiany\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, atlas_dir, atlas_list):\n",
    "        \"\"\"\n",
    "        Initialize workflow nodes\n",
    "        \"\"\"\n",
    "        self.atlas_dir = atlas_dir\n",
    "        self.atlas_list = atlas_list\n",
    "        \n",
    "    def create_nodes(self):\n",
    "        self.PostProcNodes = nodes.PostProcNodes()\n",
    "        \n",
    "    def connect_nodes(self, wf_name=\"connectomes\"):\n",
    "        \"\"\"\n",
    "        Connect post processing nodes and create workflow\n",
    "        \"\"\"\n",
    "        self.workflow = Workflow(name=wf_name, base_dir)\n",
    "        self.workflow.connect(\n",
    "            [\n",
    "                (self.PostProcNodes.subject_source, self.PostProcNodes.select_files, [('subject_id', 'subject_id'),\n",
    "                                                                                      ('session_id', 'session_id')]),\n",
    "                (self.PostProcNodes.atlas_source, self.PostProcNodes.select_atlases, [('atlas_name', 'atlas_name')]),\n",
    "                (),\n",
    "            ])\n",
    "        \n",
    "    def draw_pipeline(self):\n",
    "        \"\"\"\n",
    "        Visualize workflow\n",
    "        \"\"\"\n",
    "        self.workflow.write_graph(graph2use='orig', dotfilename='postprocess.dot')\n",
    "    \n",
    "    def run_pipeline(self, parallel=None):\n",
    "        \"\"\"\n",
    "        Run nipype workflow\n",
    "        \"\"\"\n",
    "        if type(parallel) == int:\n",
    "            print(\"Running workflow with {} parallel processes\".format(parallel))\n",
    "            self.workflow.run('MultiProc', plugin_args = {'n_procs': parallel})\n",
    "        elif parallel is None:\n",
    "            print(\"Parallel processing disabled, running workflow serially\")\n",
    "            self.workflow.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracts",
   "language": "python",
   "name": "tracts"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
