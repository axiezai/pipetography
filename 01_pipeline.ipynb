{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module pipeline\n",
    "\n",
    "> Here we have the Nodes wrapping around our core functions, which can be added to a Nipype workflow to process your input images. These definitions here are our custom functions and Nipype Nodes knotted together, which creates a working pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#export\n",
    "import os, sys\n",
    "from shutil import which\n",
    "\n",
    "import pipetography.core as ppt\n",
    "\n",
    "from nipype import IdentityInterface, Function\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.pipeline import Node, MapNode, Workflow\n",
    "from nipype.interfaces.freesurfer.preprocess import ReconAll\n",
    "from nipype.interfaces.mrtrix3.utils import BrainMask, TensorMetrics, DWIExtract, MRMath\n",
    "from nipype.interfaces.mrtrix3.preprocess import MRDeGibbs, DWIBiasCorrect\n",
    "from nipype.interfaces.mrtrix3.reconst import FitTensor\n",
    "from nipype.interfaces import ants\n",
    "from nipype.interfaces import fsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A `pipeline` class that puts everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class pipeline:\n",
    "    \"\"\"\n",
    "    A class containing functionalities and inputs to the image processing pipeline.\n",
    "    For step-by-step preprocessing example instead of a connected pipeline, see `core` page.\n",
    "    \n",
    "    This pipeline makes the following assumptions:\n",
    "        - Data input is in valid BIDS format\n",
    "        - Align all anat and dwi images to MNI space via ACPC HCP procedure\n",
    "    \n",
    "    Input: \n",
    "        BIDS_dir (str): Valid BIDS directory with DWI modality\n",
    "        RPE_design (str): Reverse phase encoding design for DWI\n",
    "        Regrid (Bool): [False] Whether to resample DWI to a template image's voxel grid\n",
    "    Attributes:\n",
    "        data_dir (str): input BIDS data directory\n",
    "        sub_list (List): List of subject IDs read from the BIDS directory\n",
    "        layout (PyBIDS Layout): PyBIDS Layout for querying\n",
    "        bfiles_fsl (tuple)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, BIDS_dir=\"data\", RPE_design=\"rpe_none\", Regrid=False):\n",
    "        # Nodes are initialized with the bare minimum of inputs or default parameters\n",
    "        # Remaining inputs are either connected in the Nipype workflow or are user inputs\n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Input variables to pipeline\n",
    "        self.data_dir = BIDS_dir\n",
    "        self.RPE_design = RPE_design\n",
    "        self.Regrid=Regrid\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Generate BIDS Layout, and create subject ID list:\n",
    "        self.sub_list, self.layout = ppt.get_subs(self.data_dir)\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Create BIDS data input node:\n",
    "        self.sub_source = Node(IdentityInterface(fields=[\"subject_id\"]), name=\"data_source\")\n",
    "        # This node will create iterables from the subject ID list:\n",
    "        self.sub_source.iterables = [(\"subject_id\", self.sub_list)]\n",
    "        \n",
    "        self.dwi_file = os.path.join(\n",
    "            \"sub-{subject_id}\", \"ses-*\", \"dwi\", \"sub-{subject_id}_ses-*_dwi.nii.gz\",\n",
    "        )\n",
    "        self.b_files = os.path.join(\n",
    "            \"sub-{subject_id}\", \"ses-1\", \"dwi\", \"sub-{subject_id}_ses-1_dwi.bv*\"\n",
    "        )\n",
    "        self.sub_template = {\"dwi\": self.dwi_file, \"b_files\": self.b_files}        \n",
    "       \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # node for selecting images and bfiles\n",
    "        self.select_files = Node(\n",
    "            SelectFiles(self.sub_template, base_directory=self.data_dir), name=\"select_files\",\n",
    "        )\n",
    "        self.bfiles_input = Node(\n",
    "            Function(\n",
    "                input_names=[\"in_List\"], output_names=[\"out_path\"], function=ppt.get_bfiles_tuple,\n",
    "            ),\n",
    "            name=\"select_bfiles\",\n",
    "        )\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # ACPC HCP MNI152 alignment nodes:\n",
    "        # first define template location in FSL:\n",
    "        self.MNI_template = os.path.expandvars('$FSLDIR/data/standard/MNI152_T1_1mm.nii.gz')\n",
    "        # The following nodes creates transformation matrices and applys them to align our T1 image\n",
    "        self.reduceFOV = MapNode(fsl.utils.RobustFOV(), name=\"reduce_FOV\", iterfield=\"in_file\")\n",
    "        self.xfminverse = MapNode(fsl.utils.ConvertXFM(), name=\"transform_inverse\", iterfield=\"in_file\")\n",
    "        self.flirt = MapNode(fsl.preprocess.FLIRT(), name=\"FLIRT\", iterfield=\"in_file\")\n",
    "        self.concatxfm = MapNode(fsl.utils.ConvertXFM(), name=\"concat_transform\", iterfield=[\"in_file\", \"in_file2\"])\n",
    "        self.alignxfm = MapNode(ppt.fslaff2rigid(), name='aff2rigid', iterfield=\"in_file\")\n",
    "        self.ACPC_warp = MapNode(fsl.preprocess.ApplyWarp(), name='apply_warp', iterfield=[\"in_file\",\"premat\"])\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # FreeSurfer recon-all mainly for WM segmentation\n",
    "        # we are greedy so we just let it segment and produce everything\n",
    "        self.reconall = MapNode(ReconAll(), name='FSrecon', iterfield=\"T1_files\")\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Preparation nodes (mask, check gradient, convert):\n",
    "        # first convert our input nifti to mrtrix3 format\n",
    "        self.mrconvert = MapNode(ppt.Convert(), name='mrtrix_image', iterfield=\"in_file\")\n",
    "        # create raw DWI brain mask\n",
    "        self.createMask = MapNode(BrainMask(), name='dwi2mask', iterfield='in_file')\n",
    "        # check for gradients and create a new image with correct gradients\n",
    "        self.GradCheck = MapNode(ppt.GradCheck(), name='dwigradcheck', iterfield='in_file')\n",
    "        self.NewGradMR = MapNode(ppt.Convert(), name='mrconvert', iterfield = \"in_file\")\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Preprocessing nodes:\n",
    "        # First MPPCA denoise with dwidenoise\n",
    "        self.denoise = MapNode(ppt.dwidenoise(), name=\"denoise\", iterfield=\"in_file\")\n",
    "        # Gibb's ringing artifact removal:\n",
    "        self.ringing = MapNode(MRDeGibbs(), name=\"ringing_removal\", iterfield=\"in_file\")\n",
    "        # DWI Distortion correction with mrtrix3's dwifslpreproc:\n",
    "        self.fslpreproc = MapNode(ppt.dwipreproc(), name = \"dwifslpreproc\", iterfield=\"in_file\")\n",
    "        # Bias field correction using ANTS:\n",
    "        self.biascorrect = MapNode(ppt.BiasCorrect(), name = 'dwibiascorret', iterfield=\"in_file\")\n",
    "        # Rician background noise removal requires multiple nodes:\n",
    "        # create new gradient file because we have changed the data extensively now:\n",
    "        self.grad_info = MapNode(ppt.MrInfo(), name = 'New Gradient', iterfield = \"in_file\")\n",
    "        # get background low noise map:\n",
    "        self.low_noisem_map = MapNode(ppt.CheckNIZ(), name = 'Low Noise Map', iterfield=\"isfininte\")\n",
    "        # Compute Rician noise:\n",
    "        self.rician_noise = MapNode(ppt.RicianNoise(), name = 'Rician Noise', iterfield = \"in_file\")\n",
    "        self.check_rician = MapNode(ppt.CheckNIZ(), name = 'Noise Comparison', iterfield = \"isfinite\")\n",
    "        self.convert_rician = MapNode(ppt.Convert(), name = \"Convnert Rician\", iterfield = \"in_file\")\n",
    "        # DWI image intensity normalization:\n",
    "        # first create a DWI brain mask:\n",
    "        self.dwi_mask = MapNode(BrainMask(), name='dwi2mask', iterfield='in_file')\n",
    "        # create a FA WM mask:\n",
    "        self.fit_tensor = MapNode(FitTensor(), name='dwi2tensor', iterfield='in_file')\n",
    "        self.tensor_FA = MapNode(TensorMetrics(), name='tensor2metrics', iterfield='in_file')\n",
    "        self.wm_mask = NapNode(ppt.MRThreshold(), name = 'mrthreshold', iterfield='in_file')\n",
    "        # finally intensity normalization:\n",
    "        self.norm_intensity = MapNode(ppt.DWINormalize(), name='dwinormalise', iterfield='in_file')\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Create new B0 and DWI brain masks with updated images to prep for re-alignement to MNI space\n",
    "        self.sub_b0extract = MapNode(DWIExtract(),  name='b0extract', iterfield='in_file')\n",
    "        self.sub_b0mean = MapNode(MRMath(), name='mrmath_mean', iterfield='in_file')\n",
    "        self.sub_b0mask = MapNode(BrainMask(), name='dwi2mask', iterfield='in_file')\n",
    "        # convert from mif to nift\n",
    "        self.sub_convert_dwi = MapNode(ppt.Convert(), name=\"dwi to nii\", iterfield=\"in_file\")\n",
    "        self.sub_convert_mask = MapNode(ppt.Convert(), name=\"mask to nii\", iterfield=\"in_file\")\n",
    "        # apply mask to B0 volume:\n",
    "        self.sub_apply_mask = MapNode(fsl.ApplyMask(), name='Apply Mask', iterfield=\"in_file\")\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Align DWI to ANAT w/ ACPC HCP MNI152 space\n",
    "        # first extract the brain:\n",
    "        self.t1_bet = MapNode(fsl.preprocess.BET(), name='fsl_bet', iterfield=\"in_file\")\n",
    "        # align DWI to ANAT... get transformation matrix first\n",
    "        self.epi_reg = MapNode(fsl.epi.EpiReg(), name='fsl_epireg', iterfield='epi')\n",
    "        # convert transformation matrix to mrtrix format\n",
    "        self.acpc_xfm = MapNode(ppt.TransConvert(), name='transformconvert', iterfield=\"flirt_xfm\")\n",
    "        # apply the transformation:\n",
    "        self.apply_xfm = MapNode(ppt.MRTransform(), name='mrtransform', iterfield=\"in_file\")\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # if regrid = True, do regridding, if False, we skip these nodes in workflow:\n",
    "        # this is mainly done to isotropically upsample your DWI data\n",
    "        # Partial upsampling is not supported and not recommended\n",
    "        self.regrid = MapNode(ppt.MRRegrid(), nam='mrgrid', iterfield='in_file')\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Recreate final DWI/Reference images and their corresponding gradient/info files:\n",
    "        self.mni_b0extract = MapNode(DWIExtract(), name='b0extract', iterfield='in_file')\n",
    "        self.mni_b0mean = MapNode(MRMath(), name='mrmath_mean', iterfield='in_file')\n",
    "        self.mni_b0mask = MapNode(BrainMask(), name='dwi2mask', iterfield='in_file')\n",
    "        self.mni_convert_dwi = MapNode(ppt.Convert(), name='dwi to nii', iterfield='in_file')\n",
    "        self.mni_convert_mask  = MapNode(ppt.Convert(), name='mask to  nii', iterfield='in_file')\n",
    "        self.mni_apply_mask = MapNode(fsl.ApplyMask(), name='Apply Mask', iterfield='in_file')\n",
    "        self.mni_dwi = MapNode(ppt.Convert(), name='MNI space outputs', iterfield='in_file')\n",
    "        \n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # Lastly, registration of atlases, these atlases will define ROIs for connectome generation:\n",
    "        # These steps should be moved to post-processing pipeline\n",
    "#         self.atlas_dir = None\n",
    "#         self.atlas_names = None\n",
    "#         self.atlas_source = None\n",
    "#         self.select_atlas = None\n",
    "#         # b0 volume brain and mask extraction:\n",
    "#         self.b0extract = MapNode(DWIExtract(), name=\"dwiextract\", iterfield=\"in_file\")\n",
    "#         self.b0mean = MapNode(MRMath(), name=\"mrmath\", iterfield=\"in_file\")\n",
    "#         self.fsl_bet = MapNode(fsl.BET(), name=\"brain_extraction\", iterfield=\"in_file\")\n",
    "#         # ants coregistration:\n",
    "#         self.linear_coreg = MapNode(\n",
    "#             ants.Registration(), name=\"linear_registration\", iterfield=\"fixed_image\",\n",
    "#         )\n",
    "#         self.nonlinear_coreg = MapNode(\n",
    "#             ants.Registration(), name=\"nonlinear_registration\", iterfield=\"fixed_image\",\n",
    "#         )\n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # data sink is the output directory of the entire pipeline:\n",
    "        self.datasink = Node(DataSink(), name=\"datasink\")\n",
    "        # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ #\n",
    "        # workflow for nipype, this will be defined after connecting the working nodes\n",
    "        self.workflow = None\n",
    "    \n",
    "        \n",
    "    def check_environment(self):\n",
    "        \"\"\"\n",
    "        Check your computing environment for FSL environment variables `FSLOUTPUTTYPE` and `FSLDIR`\n",
    "        Check if ANTs PATH is included in your environment\n",
    "        Check if mrtrix3 is in your PATH\n",
    "        Check your freesurfer envrionment variable\n",
    "        Looks for matlab compiler run time in freesurfer directory\n",
    "        \"\"\"\n",
    "        assert \"FSLOUTPUTTYPE\" in os.environ\n",
    "        assert \"FSLDIR\" in os.environ\n",
    "        if \"FSLOUTPUTTYPE\" in os.environ:\n",
    "            print(\"FSLOUTPUTTYPE is valid\")\n",
    "        else:\n",
    "            sys.exit(\"FSLOUTPUTTYPE is not defined, make sure FSL is configured!\")\n",
    "\n",
    "        if \"FSLDIR\" in os.environ:\n",
    "            print(\"FSLDIR is valid\")\n",
    "        else:\n",
    "            sys.exit(\"FSLOUTPUTTYPE is not defined, make sure FSL is configured!\")\n",
    "        \n",
    "        #ANTS:\n",
    "        assert \"ANTSPATH\" in os.environ\n",
    "        print(\"ANTS is valid\")\n",
    "        #mrtrix:\n",
    "        assert which('mrview') is not None\n",
    "        print(\"mrtrix3 is valid\")\n",
    "        return None\n",
    "    \n",
    "        \n",
    "    def set_datasink(self):\n",
    "        \"\"\"\n",
    "        Set directory where output files will be placed\n",
    "        \"\"\"\n",
    "        output_dir = str(input('Please indicate an output directory: '))\n",
    "        self.datasink.inputs.base_directory = output_dir\n",
    "    \n",
    "    \n",
    "    def reduceFOV_inputs(\n",
    "        self,\n",
    "        xfm_mat=\"roi2full.mat\",\n",
    "        out_vol=\"robustfov.nii.gz\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs to FSL's robustfov command: robustfov -i [input] -m [matrix name] -r [volume name]\n",
    "        \"\"\"\n",
    "        self.reduceFOV.inputs.out_transform = xfm_mat\n",
    "        self.reduceFOV.inputs.out_roi = out_vol\n",
    "    \n",
    "    \n",
    "    def flirt_inputs(\n",
    "        self,\n",
    "        template=self.MNI_template,\n",
    "        method='spline',\n",
    "        mat_file='roi2std.mat',\n",
    "        out_file='acpc_mni.nii.gz'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs to FSL's FLIRT:\n",
    "        -Iterable input is robustfov.nii.gz (robustFOV's output)\n",
    "        \"\"\"\n",
    "        self.FLIRT.inputs.reference = template\n",
    "        self.FLIRT.inputs.interp = method\n",
    "        self.FLIRT.inputs.out_matrix_file = mat_file\n",
    "        self.FLIRT.inputs.out_file = out_file\n",
    "    \n",
    "    \n",
    "    def xfminverse_inputs(\n",
    "        self,\n",
    "        inverse=True,\n",
    "        out_file='full2roi.mat'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs to FSL's convert_xfm command: convert_xfm -omat [filename] -inverse [input]\n",
    "        \"\"\"\n",
    "        self.xfminverse.inputs.out_file = out_file\n",
    "        self.xfminverse.inputs.invert_xfm = inverse\n",
    "    \n",
    "    \n",
    "    def concatxfm_inputs(\n",
    "        self,\n",
    "        concat = True,\n",
    "        omat = 'full2std.mat'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs for transformation matrix concatenation node\n",
    "        The matrices being concatenated are 1) flirt's output mat file  (in_file2) and 2) inverse mat file from the first convert_xfm (in_file)\n",
    "        \"\"\"\n",
    "        self.concatxfm.inputs.concat_xfm=True\n",
    "        self.concatxfm.inputs.out_file=omat\n",
    "        \n",
    "    \n",
    "    def aff2rigid_inputs(\n",
    "        self,\n",
    "        out_file='outputmatrix'\n",
    "    ):\n",
    "        self.alignxfm.inputs.out_file=out_file\n",
    "    \n",
    "    \n",
    "    def acpcwarp_inputs(\n",
    "        self,\n",
    "        out_file='acpc_t1.nii.gz',\n",
    "        rel_warp = True,\n",
    "        method = 'spline',\n",
    "        template = self.MNI_template,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs for FSL's `applywarp`\n",
    "        We want spline interpolation, relative warp field, premat = aff2rigid mat output.\n",
    "        \"\"\"\n",
    "        self.ACPC_warp.inputs.out_file = out_file\n",
    "        self.ACPC_warp.inputs.relwarp = rel_warp\n",
    "        self.ACPC_warp.inputs.interp = method\n",
    "        self.ACPC_warp.inputs.ref_file = template\n",
    "        \n",
    "    \n",
    "    # Freesurfer recon-all\n",
    "    def recon_inputs(\n",
    "        self,\n",
    "        hipp = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        We will multiproc the entire workflow, so parallel here is not recommended.\n",
    "        Only recommend parallel for single subject runs or group level runs.\n",
    "        \"\"\"\n",
    "        self.reconall.inputs.directive='all' # we always want recon-all -all\n",
    "        self.reconall.inputs.subjects_dir = self.BIDS_dir+'/derivatives/freesurfer'\n",
    "        self.reconall.inputs.hippocampal_subfields_T1 = True # we want this for compeleteness of data\n",
    "        #self.reconall.inputs.subject_id\n",
    "        \n",
    "    \n",
    "    # Convert to mrtrix3 file format:\n",
    "    def mrconvert_inputs(\n",
    "        self,\n",
    "        out_file = 'raw_dwi.mif',\n",
    "        export_mrtrix3_grad = True,\n",
    "        out_bfile = 'raw_dwi.b',\n",
    "        force = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Convert a DWI to mrtrix3's image format and gradient files\n",
    "        Workflow connected inputs not listed here:\n",
    "            - grad_fsl - gradient files in fsl format\n",
    "            - in_file - raw dwi that you want to convert to mrtrix3 format\n",
    "        \"\"\"\n",
    "        self.mrconvert.inputs.out_file = out_file\n",
    "        self.mrconvert.inputs.export_grad = export_mrtrix3_grad\n",
    "        self.mrconvert.inputs.out_bfile = out_bfile\n",
    "        self.mrconvert.inputs.force = force\n",
    "    \n",
    "    \n",
    "    # DWI Preprocessing mask creation:\n",
    "    def createMask_inputs(\n",
    "        self,\n",
    "        out_file = 'b0_brain_mask.mif'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create brain mask from B0 DWI volumes for preprocessing tasks.\n",
    "        in_file will be connected in workflow\n",
    "        \"\"\"\n",
    "        self.createMask.inputs.out_file = out_file\n",
    "        \n",
    "    \n",
    "    # gradient check and create files with corrected gradient:\n",
    "    def GradCheck_inputs(\n",
    "        self,\n",
    "        out_bfile = 'corrected.b',\n",
    "        export_mrtrix3_grad = True,\n",
    "        force = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Set inputs to dwigradcheck\n",
    "        Workflow inputs:\n",
    "            in_file = input from mrconvert Node \n",
    "            mask_file = input from createMask Node\n",
    "            grad_file = input from mrconvert Node\n",
    "        \"\"\"\n",
    "        self.GradCheck.inputs.export_grad = export_mrtrix3_grad\n",
    "        self.GradCheck.inputs.out_bfile = out_bfile\n",
    "        self.GradCheck.inputs.force = force\n",
    "    \n",
    "    \n",
    "    # Create new dwi image with corrected gradients:\n",
    "    def NewGradMR_inputs(\n",
    "        self,\n",
    "        out_file = 'cdwi.mif'\n",
    "        force = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs for creating gradient corrected MR\n",
    "        Outputs a correced dwi file\n",
    "        Workflow connected inputs:\n",
    "            in_file - output from mrconvert Node\n",
    "            grad_file - output from GradCheck Node (out_bfile)\n",
    "        \"\"\"\n",
    "        self.NewGradMR.inputs.out_file = out_file\n",
    "        self.NewGradMR.inputs.force = force\n",
    "        \n",
    "    \n",
    "    # MPPCA Denoise with dwidenoise from mrtrix3\n",
    "    def denoise_inputs(\n",
    "        self,\n",
    "        out_file=\"denoised.mif\",\n",
    "        noise_file=\"noise_map.mif\",\n",
    "        force=True,\n",
    "        quiet=False,\n",
    "        nthreads=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Set inputs to denoise node\n",
    "        \"\"\"\n",
    "        self.denoise.inputs.out_file = out_file\n",
    "        self.denoise.inputs.noise = noise_file\n",
    "        self.denoise.inputs.force = force\n",
    "\n",
    "        if quiet is True:\n",
    "            self.denoise.inputs.quiet = \"-quiet\"\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if nthreads is None:\n",
    "            pass\n",
    "        elif type(nthreads) == int:\n",
    "            self.denoise.inputs.nthreads = nthreads\n",
    "        else:\n",
    "            sys.exit(\"denoise nthreads input has to be an integer\")\n",
    "\n",
    "            \n",
    "    # Gibbs ringing artifact removal\n",
    "    def DeGibbs_inputs(self, out_file=\"ringing_removed.nii.gz\"):\n",
    "        \"\"\"\n",
    "        Set inputs to ringing removal node\n",
    "        \"\"\"\n",
    "        self.ringing.inputs.out_file = out_file\n",
    "\n",
    "    \n",
    "    # dwifslpreproc - Distortion correction with mrtrix3\n",
    "    def mrt_preproc_inputs(\n",
    "        self,\n",
    "        rpe_options,\n",
    "        pe_dir,\n",
    "        eddy_options,\n",
    "        readout = \n",
    "        force=True,\n",
    "        out_file=\"preproc.nii.gz\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Set inputs to mrtrix3's dwipreproc.\n",
    "        Arguments:\n",
    "            out_file: name of output file\n",
    "            rpe_options (str): specify phase encoding design -rpe_+ 'none/pair/all/header'\n",
    "            pe_dir (str): phase encoding direction(i,j,k)/(0,1,2)/(RL,PA,IS), can be extracted from JSON & BIDS Layout\n",
    "            eddy_options (str): '\"--slm=linear \"' aditional commandline options to eddy command, with at least 1  space\n",
    "            RO_time = '-readout_time' TotalReadoutTime from BIDS json. Defaults to 0.1 if not provided\n",
    "        \"\"\"\n",
    "        self.mrt_preproc.inputs.out_file = out_file\n",
    "        self.mrt_preproc.inputs.rpe_options = rpe_options  # -rpe_none\n",
    "        self.mrt_preproc.inputs.pe_dir = pe_dir\n",
    "        self.mrt_preproc.inputs.eddy_options = eddy_options\n",
    "    \n",
    "    \n",
    "    def ants_bfc_inputs(self, out_file=\"biasfieldcorrected.nii.gz\", dims=4):\n",
    "        \"\"\"\n",
    "        Set inputs to ANTs bias field correction node, currently incomplete\n",
    "        \"\"\"\n",
    "        self.ants_bfc.inputs.out_file = out_file\n",
    "        self.ants_bfc.inputs.dims = dims\n",
    "\n",
    "    \n",
    "\n",
    "    def atlas_inputs(self, atlas_dir, atlas_names):\n",
    "        \"\"\"\n",
    "        Input where to load atlases for ROIs and co-registration\n",
    "        \"\"\"\n",
    "        self.atlas_dir = atlas_dir\n",
    "        self.atlas_names = atlas_names\n",
    "        atlas_template = {\"atlas\": atlas_dir + \"{file_name}\"}\n",
    "        self.atlas_source = Node(IdentityInterface(fields=[\"atlas_name\"]), name=\"atlas_source\")\n",
    "        self.atlas_source.iterables = [(\"atlas_name\"), self.atlas_names]\n",
    "        self.select_atlas = Node(SelectFiles(atlas_template), name=\"select_atlases\")\n",
    "        self.select_atlas.base_directory = self.atlas_dir\n",
    "\n",
    "    def b0extract_inputs(self, out_file=\"dwi_b0.nii.gz\", bzero=True):\n",
    "        self.b0extract.inputs.out_file = out_file\n",
    "        self.b0extract.inputs.bzero = bzero\n",
    "\n",
    "    def b0mean_inputs(self, out_file=\"dwi_b0_mean.nii.gz\", axis=3):\n",
    "        self.b0mean.inputs.operation = \"mean\"\n",
    "        self.b0mean.inputs.axis = axis\n",
    "        self.b0mean.inputs.out_file = out_file\n",
    "\n",
    "    def bet_inputs(\n",
    "        self, robust=True, mask=True, frac=0.25, out_file=\"dwi_brain.nii.gz\"\n",
    "    ):\n",
    "        if robust is True:\n",
    "            self.fsl_bet.inputs.robust = robust\n",
    "        else:\n",
    "            self.fsl_bet.inputs.reduce_bias = False\n",
    "\n",
    "        self.fsl_bet.inputs.mask = mask\n",
    "        self.fsl_bet.inputs.frac = 0.25\n",
    "        self.fsl_bet.inputs.out_file = out_file\n",
    "\n",
    "    def linear_coreg_inputs(\n",
    "        self,\n",
    "        dims=3,\n",
    "        prefix=\"atlas_in_dwi_affine\",\n",
    "        collapse=True,\n",
    "        transform=[\"Affine\"],\n",
    "        parameters=[(0.1,)],\n",
    "        metric=[\"MI\"],\n",
    "        weight=[1],\n",
    "        radius_bins=[64],\n",
    "        num_iter=[[500, 200, 200, 100]],\n",
    "        threshold=[1e-06],\n",
    "        window_size=[10],\n",
    "        sigmas=[[4, 2, 1, 0]],\n",
    "        units=[\"vox\"],\n",
    "        shrink=[[8, 4, 2, 1]],\n",
    "        histogram=True,\n",
    "        output=\"atlas_in_dwi_affine.nii.gz\",\n",
    "    ):\n",
    "        self.linear_coreg.inputs.dimension = dims\n",
    "        self.linear_coreg.inputs.output_transform_prefix = prefix\n",
    "        self.linear_coreg.inputs.collapse_output_transforms = collapse\n",
    "        self.linear_coreg.inputs.transforms = transform\n",
    "        self.linear_coreg.inputs.transform_parameters = parameters\n",
    "        self.linear_coreg.inputs.metric = metric\n",
    "        self.linear_coreg.inputs.metric_weight = weight  # default, value ignored by ANTS\n",
    "        self.linear_coreg.inputs.radius_or_number_of_bins = radius_bins\n",
    "        # -convergence\n",
    "        self.linear_coreg.inputs.number_of_iterations = num_iter\n",
    "        self.linear_coreg.inputs.convergence_threshold = threshold\n",
    "        self.linear_coreg.inputs.convergence_window_size = window_size\n",
    "        # -s\n",
    "        self.linear_coreg.inputs.smoothing_sigmas = sigmas\n",
    "        self.linear_coreg.inputs.sigma_units = units\n",
    "        # -f\n",
    "        self.linear_coreg.inputs.shrink_factors = shrink\n",
    "        self.linear_coreg.inputs.use_histogram_matching = histogram  # -u flag\n",
    "        self.linear_coreg.inputs.output_warped_image = output\n",
    "    \n",
    "    def nonlinear_coreg_inputs(\n",
    "        self,\n",
    "        dims=3,\n",
    "        prefix=\"atlas_in_dwi_syn\",\n",
    "        collapse=True,\n",
    "        transform=['SyN'],\n",
    "        parameters=[(0.1,)],\n",
    "        metric=[\"MI\"],\n",
    "        weight=[1],\n",
    "        radius_bins=[64],\n",
    "        num_iter=[[500, 200, 200, 100]],\n",
    "        threshold=[1e-06],\n",
    "        window_size=[10],\n",
    "        sigmas=[[4, 2, 1, 0]],\n",
    "        units=[\"vox\"],\n",
    "        shrink=[[8, 4, 2, 1]],\n",
    "        histogram=True,\n",
    "        output=\"atlas_in_dwi_syn.nii.gz\",\n",
    "    ):\n",
    "        self.nonlinear_coreg.inputs.dimension = dims\n",
    "        self.nonlinear_coreg.inputs.output_transform_prefix = prefix\n",
    "        self.nonlinear_coreg.inputs.collapse_output_transforms = collapse\n",
    "        self.nonlinear_coreg.inputs.transforms = transform\n",
    "        self.nonlinear_coreg.inputs.transform_parameters = parameters\n",
    "        self.nonlinear_coreg.inputs.metric = metric\n",
    "        self.nonlinear_coreg.inputs.metric_weight = weight  # default, value ignored by ANTS\n",
    "        self.nonlinear_coreg.inputs.radius_or_number_of_bins = radius_bins\n",
    "        # -convergence\n",
    "        self.nonlinear_coreg.inputs.number_of_iterations = num_iter\n",
    "        self.nonlinear_coreg.inputs.convergence_threshold = threshold\n",
    "        self.nonlinear_coreg.inputs.convergence_window_size = window_size\n",
    "        # -s\n",
    "        self.nonlinear_coreg.inputs.smoothing_sigmas = sigmas\n",
    "        self.nonlinear_coreg.inputs.sigma_units = units\n",
    "        # -f\n",
    "        self.nonlinear_coreg.inputs.shrink_factors = shrink\n",
    "        self.nonlinear_coreg.inputs.use_histogram_matching = histogram  # -u flag\n",
    "        self.nonlinear_coreg.inputs.output_warped_image = output\n",
    "\n",
    "    def connect_nodes(self, wf_name=\"pipetography\"):\n",
    "        # a workflow that will be filled in:\n",
    "        self.workflow = Workflow(name=wf_name, base_dir=self.data_dir + \"/derivatives\",)\n",
    "        # folder -> select files\n",
    "        self.workflow.connect(\n",
    "            self.sub_source, \"subject_id\", self.select_files, \"subject_id\",\n",
    "        )\n",
    "        # select files -> obtain bvec/bval path as tuple\n",
    "        self.workflow.connect(\n",
    "            self.select_files, \"b_files\", self.bfiles_input, \"in_List\",\n",
    "        )\n",
    "        # selected dwi -> denoise\n",
    "        self.workflow.connect(\n",
    "            self.select_files, \"dwi\", self.denoise, \"in_file\",\n",
    "        )\n",
    "        # denoised output -> datasink\n",
    "        self.workflow.connect(\n",
    "            self.denoise, \"out_file\", self.datasink, \"preproc.@denoised\",\n",
    "        )\n",
    "        # denoised output -> ringing removal\n",
    "        self.workflow.connect(\n",
    "            self.denoise, \"out_file\", self.ringing, \"in_file\",\n",
    "        )\n",
    "        # ringing removal -> datasink\n",
    "        self.workflow.connect(\n",
    "            self.ringing, \"out_file\", self.datasink, \"preproc.@ringing_removed\",\n",
    "        )\n",
    "        # ringing removal -> bias field correction\n",
    "        self.workflow.connect(\n",
    "            self.ringing, \"out_file\", self.ants_bfc, \"in_file\",\n",
    "        )\n",
    "        # bias field corrected -> datasink\n",
    "        self.workflow.connect(\n",
    "            self.ants_bfc, \"out_file\", self.datasink, \"preproc.@biasCorrected\",\n",
    "        )\n",
    "        # bias field corrected -> eddy correction\n",
    "        self.workflow.connect(\n",
    "            self.ants_bfc, \"out_file\", self.mrt_preproc, \"in_file\",\n",
    "        )\n",
    "        # grad_fsl for eddy correction:\n",
    "        self.workflow.connect(\n",
    "            self.bfiles_input, \"out_path\", self.mrt_preproc, \"grad_fsl\",\n",
    "        )\n",
    "        # preproc --> data sink\n",
    "        self.workflow.connect(\n",
    "            self.mrt_preproc, \"out_file\", self.datasink, \"preproc.@preproced\",\n",
    "        )\n",
    "        # b0 extractions:\n",
    "        self.workflow.connect(\n",
    "            self.mrt_preproc, \"out_file\", self.b0extract, \"in_file\",\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.bfiles_input, \"out_path\", self.b0extract, \"grad_fsl\",\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.b0extract, \"out_file\", self.b0mean, \"in_file\",\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.b0mean, \"out_file\", self.fsl_bet, \"in_file\",\n",
    "        )\n",
    "        # extractions --> datasink:\n",
    "        self.workflow.connect(\n",
    "            self.b0extract, \"out_file\", self.datasink, \"extraction.@volume\",\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.b0mean, \"out_file\", self.datasink, \"extraction.@mean\",\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.fsl_bet, \"mask_file\", self.datasink, \"extraction.@BET_mask\",\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.fsl_bet, \"out_file\", self.datasink, \"extraction.@BET_image\",\n",
    "        )\n",
    "        # coregistration of atlases to DWI, select atlases as moving image\n",
    "        self.workflow.connect(\n",
    "            self.atlas_source, 'atlas_name', self.select_atlas, 'file_name'\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.select_atlas, 'atlas', self.linear_coreg, 'moving_image'\n",
    "        )\n",
    "        # use extracted brain as target brain for co-registration, no dwi noise\n",
    "        self.workflow.connect(\n",
    "            self.fsl_bet, 'out_file', self.linear_coreg, 'fixed_image'\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.fsl_bet, 'out_file', self.nonlinear_coreg, 'fixed_image'\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.linear_coreg, 'warped_image', self.nonlinear_coreg, 'moving_image'\n",
    "        )\n",
    "        # coreg --> datasink\n",
    "        self.workflow.connect(\n",
    "            self.linear_coreg, 'warped_image', self.datasink, 'coreg.@affine_atlas'\n",
    "        )\n",
    "        self.workflow.connect(\n",
    "            self.nonlinear_coreg, 'warped_image', self.datasink, 'coreg.@syn_atlas'\n",
    "        )\n",
    "        #\n",
    "        \n",
    "    def default_setup(self, phase_encoding_design = \"-rpe_none\", phase_encoding_dir = \"j-\"):\n",
    "        \"\"\"\n",
    "        Set up pipeline parameters with all default settings blindly\n",
    "        You want to use this for a quick test run. Then tweak parameters after examining all output images\n",
    "        Note that this assumes input options for eddy current correction as \"--slm=linear\" \n",
    "        Arguments:\n",
    "            phase_encoding_design (str): defaults to \"-rpe_none\", or no reverse phase encoding volume, eddy current and motion correction only, other options include \"rpe_pair, rpe_all, rpe_header\"\n",
    "            phase_encoding_dir (str): defaults to \"j-\", or anterior to posterior. (Options: i,j,k or RL, PA, IS)\n",
    "        \"\"\"\n",
    "        self.denoise_inputs(force = True, quiet = True)\n",
    "        self.DeGibbs_inputs()\n",
    "        self.ants_bfc_inputs()\n",
    "        self.mrt_preproc_inputs(\n",
    "            rpe_options=phase_encoding_design,\n",
    "            pe_dir=phase_encoding_dir,\n",
    "            eddy_options='\"--slm=linear --verbose\"',\n",
    "            nthreads=4,\n",
    "            )\n",
    "        self.b0extract_inputs()\n",
    "        self.b0mean_inputs()\n",
    "        self.bet_inputs()\n",
    "        self.atlas_inputs(atlas_dir=str(input('Please indicate directory with atlas volumes: ')), atlas_names=list(input('Please indicate list of selected atlas names: ')))\n",
    "        self.linear_coreg_inputs() #defaults\n",
    "        self.nonlinear_coreg_inputs() #defaults\n",
    "        self.connect_nodes(wf_name = 'default_workflow')\n",
    "        return print(self.__dict__)\n",
    "        \n",
    "    def draw_pipeline(self):\n",
    "        self.workflow.write_graph(graph2use='orig', dotfilename = 'pipetography.dot')\n",
    "        \n",
    "    def run_pipeline(self, parallel = True):\n",
    "        if parallel is True:\n",
    "            processes = int(input('Number of Processes: '))\n",
    "            self.workflow.run('MultiProc', plugin_args = {'n_procs': processes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating layout of data directory, might take a while if there are a lot of subjects\n"
     ]
    }
   ],
   "source": [
    "#create pipeline:\n",
    "preproc_dwi = pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if our environment is properly set-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSLOUTPUTTYPE is valid\n",
      "FSLDIR is valid\n",
      "ANTS is valid\n",
      "mrtrix3 is valid\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "preproc_dwi.check_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please indicate an output directory:  'output'\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "#set output destination:\n",
    "preproc_dwi.set_datasink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at what's in the `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': 'data',\n",
       " 'sub_list': ['11048'],\n",
       " 'layout': BIDS Layout: ...ers/xxie/lab/pipetography/data | Subjects: 1 | Sessions: 1 | Runs: 0,\n",
       " 'dwi_file': 'sub-{subject_id}/ses-*/dwi/sub-{subject_id}_ses-*_dwi.nii.gz',\n",
       " 'b_files': 'sub-{subject_id}/ses-1/dwi/sub-{subject_id}_ses-1_dwi.bv*',\n",
       " 'sub_template': {'dwi': 'sub-{subject_id}/ses-*/dwi/sub-{subject_id}_ses-*_dwi.nii.gz',\n",
       "  'b_files': 'sub-{subject_id}/ses-1/dwi/sub-{subject_id}_ses-1_dwi.bv*'},\n",
       " 'sub_source': data_source,\n",
       " 'select_files': select_files,\n",
       " 'bfiles_input': select_bfiles,\n",
       " 'denoise': denoise,\n",
       " 'ringing': ringing_removal,\n",
       " 'ants_bfc': ants_bias_correct,\n",
       " 'mrt_preproc': mrtrix3_preproc,\n",
       " 'atlas_dir': None,\n",
       " 'atlas_names': None,\n",
       " 'atlas_source': None,\n",
       " 'select_atlas': None,\n",
       " 'b0extract': dwiextract,\n",
       " 'b0mean': mrmath,\n",
       " 'fsl_bet': brain_extraction,\n",
       " 'linear_coreg': linear_registration,\n",
       " 'nonlinear_coreg': nonlinear_registration,\n",
       " 'datasink': datasink,\n",
       " 'workflow': None}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc_dwi.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set up preprocessing pipeline with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please indicate directory with atlas volumes:  '/Users/xxie/lab/atlases'\n",
      "Please indicate list of selected atlas names:  ['BN_Atlas_246_2mm.nii','DK_atlas86_1mm.nii']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': 'data', 'sub_list': ['11048'], 'layout': BIDS Layout: ...ers/xxie/lab/pipetography/data | Subjects: 1 | Sessions: 1 | Runs: 0, 'dwi_file': 'sub-{subject_id}/ses-*/dwi/sub-{subject_id}_ses-*_dwi.nii.gz', 'b_files': 'sub-{subject_id}/ses-1/dwi/sub-{subject_id}_ses-1_dwi.bv*', 'sub_template': {'dwi': 'sub-{subject_id}/ses-*/dwi/sub-{subject_id}_ses-*_dwi.nii.gz', 'b_files': 'sub-{subject_id}/ses-1/dwi/sub-{subject_id}_ses-1_dwi.bv*'}, 'sub_source': default_workflow.data_source, 'select_files': default_workflow.select_files, 'bfiles_input': default_workflow.select_bfiles, 'denoise': default_workflow.denoise, 'ringing': default_workflow.ringing_removal, 'ants_bfc': default_workflow.ants_bias_correct, 'mrt_preproc': default_workflow.mrtrix3_preproc, 'atlas_dir': \"'/Users/xxie/lab/atlases'\", 'atlas_names': ['[', \"'\", 'B', 'N', '_', 'A', 't', 'l', 'a', 's', '_', '2', '4', '6', '_', '2', 'm', 'm', '.', 'n', 'i', 'i', \"'\", ',', \"'\", 'D', 'K', '_', 'a', 't', 'l', 'a', 's', '8', '6', '_', '1', 'm', 'm', '.', 'n', 'i', 'i', \"'\", ']'], 'atlas_source': default_workflow.atlas_source, 'select_atlas': default_workflow.select_atlases, 'b0extract': default_workflow.dwiextract, 'b0mean': default_workflow.mrmath, 'fsl_bet': default_workflow.brain_extraction, 'linear_coreg': default_workflow.linear_registration, 'nonlinear_coreg': default_workflow.nonlinear_registration, 'datasink': default_workflow.datasink, 'workflow': default_workflow}\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "#usage\n",
    "preproc_dwi.default_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR we can fill this in one by one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's tell the pipeline where we have atlas volumes and which ones to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "preproc_dwi.atlas_inputs(atlas_dir = '/Users/xxie/lab/atlases', atlas_names = ['BN_Atlas_246_2mm.nii','DK_atlas86_1mm.nii'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we give inputs to the `denoise` Node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "preproc_dwi.denoise_inputs(force = True, quiet = True)\n",
    "# we kept everything else default, like output names and number of threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibbs ringing removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "preproc_dwi.DeGibbs_inputs() # keep it default\n",
    "print('Output file name is ' + preproc_dwi.ringing.inputs.out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANTs Bias Field Correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "preproc_dwi.ants_bfc_inputs() # keep it default:\n",
    "preproc_dwi.ants_bfc.inputs.print_traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`in_file` is undefined because we will feed it this information once we connect our individual functions.\n",
    "\n",
    "Next let's set up eddy current/motion correction, we need to tell the pipeline the phase encoding settings and inputs to eddy algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "preproc_dwi.mrt_preproc_inputs(\n",
    "    rpe_options=\"-rpe_none\",\n",
    "    pe_dir=\"j-\",\n",
    "    eddy_options='\"--slm=linear --verbose\"',\n",
    "    nthreads=4,\n",
    ")\n",
    "preproc_dwi.mrt_preproc.inputs.print_traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad_fsl` and `in_file` will be input via connected workflow later\n",
    "\n",
    "Next, we set-up brain extraction from B0 volumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "# Extract b0 volumes:\n",
    "preproc_dwi.b0extract_inputs() #default\n",
    "# Create average B0 volume:\n",
    "preproc_dwi.b0mean_inputs() #default\n",
    "# Extract brain from B0 average volume:\n",
    "preproc_dwi.bet_inputs() #defaults again, using FSL's BET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANTs registration set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "preproc_dwi.linear_coreg_inputs() #defaults\n",
    "preproc_dwi.nonlinear_coreg_inputs() #defaults\n",
    "preproc_dwi.nonlinear_coreg.inputs.print_traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the default settings may not be optimal for your dataset, run the processing for a test image and see the intermediate results and then tweak the available inputs to improve the output image quality.\n",
    "\n",
    "Now that our pre-processing Nodes are properly setup, we can connect and create a workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "preproc_dwi.connect_nodes(wf_name = 'pipetography_workflow')\n",
    "preproc_dwi.draw_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the final workflow we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "from IPython.display import Image\n",
    "Image('data/derivatives/test_run/pipetography_detailed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracts",
   "language": "python",
   "name": "tracts"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
